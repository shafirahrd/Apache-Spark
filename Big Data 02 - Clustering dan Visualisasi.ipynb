{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source : https://www.kaggle.com/chicago/chicago-food-inspections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This information is derived from inspections of restaurants and other food establishments in Chicago from January 1, 2010 to the present. Inspections are performed by staff from the Chicago Department of Public Healthâ€™s Food Protection Program using a standardized procedure. The results of the inspection are inputted into a database, then reviewed and approved by a State of Illinois Licensed Environmental Health Practitioner (LEHP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import findspark to read SPARK_HOME and HADOOP_HOME\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required library\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000024CC00CBCC0>\n"
     ]
    }
   ],
   "source": [
    "# Print Spark object ID\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data & Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset Business Licenses & Owners\n",
    "data = spark.read.csv(\"food-inspections.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183905"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop missing value\n",
    "df = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133057"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "#Convert column `Latitude` and `Longitude` data type to float\n",
    "df = df.withColumn(\"Latitude\", df[\"Latitude\"].cast(\"float\"))\n",
    "df = df.withColumn(\"Longitude\", df[\"Longitude\"].cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Inspection ID: integer (nullable = true)\n",
      " |-- DBA Name: string (nullable = true)\n",
      " |-- AKA Name: string (nullable = true)\n",
      " |-- License #: integer (nullable = true)\n",
      " |-- Facility Type: string (nullable = true)\n",
      " |-- Risk: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Zip: integer (nullable = true)\n",
      " |-- Inspection Date: timestamp (nullable = true)\n",
      " |-- Inspection Type: string (nullable = true)\n",
      " |-- Results: string (nullable = true)\n",
      " |-- Violations: string (nullable = true)\n",
      " |-- Latitude: float (nullable = true)\n",
      " |-- Longitude: float (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"inspect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------------+------------------+\n",
      "| Latitude| Longitude|           Risk|           Results|\n",
      "+---------+----------+---------------+------------------+\n",
      "|41.692104| -87.65434|  Risk 1 (High)|              Pass|\n",
      "| 41.90856|-87.646454|  Risk 1 (High)|Pass w/ Conditions|\n",
      "|   41.986| -87.78753|  Risk 1 (High)|Pass w/ Conditions|\n",
      "|41.909847| -87.73744|  Risk 1 (High)|Pass w/ Conditions|\n",
      "|  41.9655| -87.65825|Risk 2 (Medium)|              Fail|\n",
      "| 41.89668|  -87.7725|  Risk 1 (High)|Pass w/ Conditions|\n",
      "| 41.96556| -87.65326|  Risk 1 (High)|Pass w/ Conditions|\n",
      "|41.751846| -87.58615|  Risk 1 (High)|Pass w/ Conditions|\n",
      "| 41.70404| -87.56588|  Risk 1 (High)|Pass w/ Conditions|\n",
      "|41.944996| -87.64927|  Risk 1 (High)|Pass w/ Conditions|\n",
      "|41.878048| -87.64774|  Risk 1 (High)|Pass w/ Conditions|\n",
      "|41.890137| -87.63065|Risk 2 (Medium)|Pass w/ Conditions|\n",
      "|41.757504| -87.56528|  Risk 1 (High)|              Fail|\n",
      "|41.819443|-87.665375|   Risk 3 (Low)|              Fail|\n",
      "|41.961594| -87.67667|  Risk 1 (High)|Pass w/ Conditions|\n",
      "|41.880318| -87.75075|Risk 2 (Medium)|Pass w/ Conditions|\n",
      "|41.884224|-87.651695|  Risk 1 (High)|Pass w/ Conditions|\n",
      "| 41.90312|-87.666336|  Risk 1 (High)|Pass w/ Conditions|\n",
      "| 41.80041| -87.58866|  Risk 1 (High)|Pass w/ Conditions|\n",
      "|41.691486| -87.72018|Risk 2 (Medium)|Pass w/ Conditions|\n",
      "+---------+----------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Location of Business along with it's food risk and inspection result\n",
    "query = spark.sql(\"SELECT DISTINCT Latitude, Longitude, Risk, Results FROM inspect\")\n",
    "\n",
    "query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#Features assembler\n",
    "vecAssembler = VectorAssembler(inputCols=[\"Latitude\", \"Longitude\"], outputCol=\"features\")\n",
    "dfassembled = vecAssembler.transform(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------------+------------------+--------------------+\n",
      "| Latitude| Longitude|           Risk|           Results|            features|\n",
      "+---------+----------+---------------+------------------+--------------------+\n",
      "|41.692104| -87.65434|  Risk 1 (High)|              Pass|[41.6921043395996...|\n",
      "| 41.90856|-87.646454|  Risk 1 (High)|Pass w/ Conditions|[41.9085617065429...|\n",
      "|   41.986| -87.78753|  Risk 1 (High)|Pass w/ Conditions|[41.9860000610351...|\n",
      "|41.909847| -87.73744|  Risk 1 (High)|Pass w/ Conditions|[41.9098472595214...|\n",
      "|  41.9655| -87.65825|Risk 2 (Medium)|              Fail|[41.9654998779296...|\n",
      "| 41.89668|  -87.7725|  Risk 1 (High)|Pass w/ Conditions|[41.8966789245605...|\n",
      "| 41.96556| -87.65326|  Risk 1 (High)|Pass w/ Conditions|[41.9655609130859...|\n",
      "|41.751846| -87.58615|  Risk 1 (High)|Pass w/ Conditions|[41.7518463134765...|\n",
      "| 41.70404| -87.56588|  Risk 1 (High)|Pass w/ Conditions|[41.7040405273437...|\n",
      "|41.944996| -87.64927|  Risk 1 (High)|Pass w/ Conditions|[41.9449958801269...|\n",
      "|41.878048| -87.64774|  Risk 1 (High)|Pass w/ Conditions|[41.8780479431152...|\n",
      "|41.890137| -87.63065|Risk 2 (Medium)|Pass w/ Conditions|[41.89013671875,-...|\n",
      "|41.757504| -87.56528|  Risk 1 (High)|              Fail|[41.7575035095214...|\n",
      "|41.819443|-87.665375|   Risk 3 (Low)|              Fail|[41.8194427490234...|\n",
      "|41.961594| -87.67667|  Risk 1 (High)|Pass w/ Conditions|[41.9615936279296...|\n",
      "|41.880318| -87.75075|Risk 2 (Medium)|Pass w/ Conditions|[41.8803176879882...|\n",
      "|41.884224|-87.651695|  Risk 1 (High)|Pass w/ Conditions|[41.8842239379882...|\n",
      "| 41.90312|-87.666336|  Risk 1 (High)|Pass w/ Conditions|[41.9031181335449...|\n",
      "| 41.80041| -87.58866|  Risk 1 (High)|Pass w/ Conditions|[41.8004112243652...|\n",
      "|41.691486| -87.72018|Risk 2 (Medium)|Pass w/ Conditions|[41.6914863586425...|\n",
      "+---------+----------+---------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfassembled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "#Training model\n",
    "kmeans = KMeans().setK(5).setSeed(1)\n",
    "model = kmeans.fit(dfassembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------------+------------------+--------------------+----------+\n",
      "| Latitude| Longitude|           Risk|           Results|            features|prediction|\n",
      "+---------+----------+---------------+------------------+--------------------+----------+\n",
      "|41.692104| -87.65434|  Risk 1 (High)|              Pass|[41.6921043395996...|         0|\n",
      "| 41.90856|-87.646454|  Risk 1 (High)|Pass w/ Conditions|[41.9085617065429...|         3|\n",
      "|   41.986| -87.78753|  Risk 1 (High)|Pass w/ Conditions|[41.9860000610351...|         1|\n",
      "|41.909847| -87.73744|  Risk 1 (High)|Pass w/ Conditions|[41.9098472595214...|         1|\n",
      "|  41.9655| -87.65825|Risk 2 (Medium)|              Fail|[41.9654998779296...|         2|\n",
      "| 41.89668|  -87.7725|  Risk 1 (High)|Pass w/ Conditions|[41.8966789245605...|         1|\n",
      "| 41.96556| -87.65326|  Risk 1 (High)|Pass w/ Conditions|[41.9655609130859...|         2|\n",
      "|41.751846| -87.58615|  Risk 1 (High)|Pass w/ Conditions|[41.7518463134765...|         0|\n",
      "| 41.70404| -87.56588|  Risk 1 (High)|Pass w/ Conditions|[41.7040405273437...|         0|\n",
      "|41.944996| -87.64927|  Risk 1 (High)|Pass w/ Conditions|[41.9449958801269...|         2|\n",
      "|41.878048| -87.64774|  Risk 1 (High)|Pass w/ Conditions|[41.8780479431152...|         3|\n",
      "|41.890137| -87.63065|Risk 2 (Medium)|Pass w/ Conditions|[41.89013671875,-...|         3|\n",
      "|41.757504| -87.56528|  Risk 1 (High)|              Fail|[41.7575035095214...|         0|\n",
      "|41.819443|-87.665375|   Risk 3 (Low)|              Fail|[41.8194427490234...|         4|\n",
      "|41.961594| -87.67667|  Risk 1 (High)|Pass w/ Conditions|[41.9615936279296...|         2|\n",
      "|41.880318| -87.75075|Risk 2 (Medium)|Pass w/ Conditions|[41.8803176879882...|         1|\n",
      "|41.884224|-87.651695|  Risk 1 (High)|Pass w/ Conditions|[41.8842239379882...|         3|\n",
      "| 41.90312|-87.666336|  Risk 1 (High)|Pass w/ Conditions|[41.9031181335449...|         3|\n",
      "| 41.80041| -87.58866|  Risk 1 (High)|Pass w/ Conditions|[41.8004112243652...|         0|\n",
      "|41.691486| -87.72018|Risk 2 (Medium)|Pass w/ Conditions|[41.6914863586425...|         4|\n",
      "+---------+----------+---------------+------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Prediction with previous model\n",
    "predictions = model.transform(dfassembled)\n",
    "predictions.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.5968021262854156\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "#Evaluate clustering by computing silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[ 41.7440957  -87.61554566]\n",
      "[ 41.93161719 -87.75760653]\n",
      "[ 41.96400966 -87.67770146]\n",
      "[ 41.88640256 -87.6475393 ]\n",
      "[ 41.80659864 -87.70610671]\n"
     ]
    }
   ],
   "source": [
    "#Centroids\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "basemap": "light-v9",
      "chartsize": "100",
      "custombasecolor": "#ffff00",
      "custombasecolorsecondary": "#ff8000",
      "handlerId": "mapView",
      "keyFields": "Latitude,Longitude",
      "kind": "simple",
      "legend": "true",
      "mapboxtoken": "pk.eyJ1IjoibWFwYm94IiwiYSI6ImNpejY4M29iazA2Z2gycXA4N2pmbDZmangifQ.-g_vE53SD2WrJ6tFX7QHmA",
      "rendererId": "mapbox",
      "valueFields": "prediction"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.15</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization using pixiedust\n",
    "import pixiedust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "mapView",
      "keyFields": "Latitude,Longitude",
      "mapboxtoken": "pk.eyJ1IjoibWFwYm94IiwiYSI6ImNpejY4M29iazA2Z2gycXA4N2pmbDZmangifQ.-g_vE53SD2WrJ6tFX7QHmA",
      "valueFields": "prediction"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">.pd_warning{display:none;}</style><div class=\"pd_warning\"><em>Hey, there's something awesome here! To see it, open this notebook outside GitHub, in a viewer like Jupyter</em></div>\n",
       "        <div class=\"pd_save is-viewer-good\" style=\"padding-right:10px;text-align: center;line-height:initial !important;font-size: xx-large;font-weight: 500;color: coral;\">\n",
       "            \n",
       "        </div>\n",
       "    <div id=\"chartFigure48ac8488\" class=\"pd_save is-viewer-good\" style=\"overflow-x:auto\">\n",
       "            \n",
       "                <div style=\"min-height: 50px;\">\n",
       "                    <div style=\"color: red;position: absolute;bottom: 0;left: 0;\"></div>\n",
       "                </div>\n",
       "            \n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
